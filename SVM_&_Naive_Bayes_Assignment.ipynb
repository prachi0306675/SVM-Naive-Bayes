{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **SVM & Naive Bayes - Assignment Questions & Answers**"
      ],
      "metadata": {
        "id": "DQJ-D8IrFuQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1. What is a Support Vector Machine (SVM), and how does it work?**\n",
        "  - A **Support Vector Machine (SVM)** is a powerful supervised machine learning algorithm used for both classification and regression tasks, though it's most commonly applied to classification. The core idea behind SVMs is to find the optimal hyperplane that best separates different classes in the feature space.\n",
        "  \n",
        "  Here's a breakdown of how it works:\n",
        "  1.  **Finding the Optimal Hyperplane:** In a 2D space, a hyperplane is simply a line. In higher dimensions, it's a flat subspace that divides the space into two regions. The goal of an SVM is to find a hyperplane that maximizes the margin between the different classes. The margin is the distance between the hyperplane and the nearest data points from each class.\n",
        "  2.  **Support Vectors:** The data points that are closest to the hyperplane and influence its position are called **support vectors**. These are the critical points that determine the optimal hyperplane and the margin.\n",
        "\n",
        "  3.  **Maximizing the Margin:** By maximizing the margin, the SVM aims to create a more robust and generalized model. A larger margin generally leads to better performance on unseen data because it provides a buffer zone between the classes.\n",
        "\n",
        "  4.  **Handling Non-linearly Separable Data:** SVMs can handle cases where the data is not linearly separable in the original feature space. This is done using the **kernel trick**. The kernel trick allows SVMs to implicitly map the data into a higher-dimensional space where it might become linearly separable, without explicitly calculating the coordinates in that higher dimension. Common kernel functions include:\n",
        "    *   **Linear Kernel:** Suitable for linearly separable data.\n",
        "    *   **Polynomial Kernel:** Maps data into a polynomial feature space.\n",
        "    *   **Radial Basis Function (RBF) Kernel:** A popular choice that can handle complex non-linear relationships.\n",
        "\n",
        "  5.  **Soft Margin SVM:** In real-world scenarios, data often contains noise or outliers, making it impossible to achieve perfect linear separation. Soft margin SVM introduces a regularization parameter (often denoted by 'C') that allows for some misclassifications or violations of the margin. A smaller 'C' allows more violations but creates a wider margin, while a larger 'C' penalizes violations more heavily, resulting in a narrower margin. This parameter helps balance the trade-off between maximizing the margin and minimizing classification errors.\n",
        "\n",
        "  In summary, SVMs work by finding the optimal hyperplane that maximizes the margin between classes, using support vectors to define this hyperplane. They can handle non-linearly separable data through the kernel trick and can tolerate some misclassifications with the soft margin approach."
      ],
      "metadata": {
        "id": "mkRCFhCAFuPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.2.Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "  - **Hard Margin SVM vs. Soft Margin SVM**\n",
        "\n",
        "  The key difference between Hard Margin and Soft Margin SVM lies in how they handle data points that are not perfectly separable or contain noise:\n",
        "\n",
        "*   **Hard Margin SVM:**\n",
        "    *   Assumes the data is **linearly separable** without any errors.\n",
        "    *   Finds a hyperplane that perfectly separates the classes with the largest possible margin.\n",
        "    *   Does not allow any data points to fall within the margin or on the wrong side of the hyperplane.\n",
        "    *   Can be very sensitive to outliers and noise, as even a single misclassified point can make it impossible to find a separating hyperplane.\n",
        "    *   Requires the data to be perfectly clean and separable, which is rarely the case in real-world scenarios.\n",
        "\n",
        "*   **Soft Margin SVM:**\n",
        "    *   Allows for some **misclassifications** or violations of the margin.\n",
        "    *   Introduces a **regularization parameter (C)** that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
        "    *   A smaller 'C' allows more violations but creates a wider margin, leading to a more generalized model but potentially more training errors.\n",
        "    *   A larger 'C' penalizes violations more heavily, resulting in a narrower margin but fewer training errors. This can lead to overfitting if 'C' is too large.\n",
        "    *   Is more robust to noise and outliers, making it suitable for real-world datasets where perfect separation is not possible.\n",
        "    *   Is the more commonly used approach in practice.\n",
        "\n",
        "  In essence, Hard Margin SVM is a stricter version that demands perfect separation, while Soft Margin SVM is more flexible and allows for some errors to achieve better generalization on noisy or non-linearly separable data. The 'C' parameter in Soft Margin SVM provides a way to tune this flexibility."
      ],
      "metadata": {
        "id": "xfPIRJ5LSsWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.**\n",
        "  - The **Kernel Trick** is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. It allows SVMs to implicitly map data into a higher-dimensional feature space where it may become linearly separable, without actually computing the coordinates of the data in that higher dimension. This is computationally less expensive than explicitly transforming the data.\n",
        "\n",
        "  Essentially, the kernel trick replaces the dot product of the transformed data points in the higher dimension with a kernel function applied to the original data points. The kernel function calculates the similarity between two data points as if they were in the higher-dimensional space.\n",
        "\n",
        "  **One example of a kernel is the Radial Basis Function (RBF) kernel.**\n",
        "\n",
        "*   **RBF Kernel:** The RBF kernel is one of the most commonly used kernels in SVMs. It is defined as:\n",
        "\n",
        "  $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$\n",
        "\n",
        "   where:\n",
        "  *   $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are two data points.\n",
        "  *   $\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2$ is the squared Euclidean distance between the two data points.\n",
        "  *   $\\gamma$ is a parameter that controls the influence of a single training example. A larger $\\gamma$ means that a single training example has a closer reach, while a smaller $\\gamma$ means a further reach.\n",
        "\n",
        "  *   **Use Case:** The RBF kernel is particularly useful when the relationship between the data points is non-linear and complex. It can create complex decision boundaries that are not possible with linear kernels. It is often used in image classification, handwriting recognition, and other tasks where the data is not linearly separable in the original feature space. The RBF kernel is a good default choice when you don't have prior knowledge about the data's structure."
      ],
      "metadata": {
        "id": "sXAgkI07SsS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.4. What is a Naïve Bayes Classifier, and why is it called “naïve”?**\n",
        "  - A **Naïve Bayes Classifier** is a probabilistic machine learning algorithm based on Bayes' Theorem. It is commonly used for classification tasks, particularly in text classification and spam filtering. Despite its simplicity, it can perform surprisingly well in many real-world applications.\n",
        "\n",
        "  Here's how it works:\n",
        "\n",
        "  1.  **Bayes' Theorem:** The classifier is based on Bayes' Theorem, which describes the probability of a hypothesis given evidence. In the context of classification, it calculates the probability that a given data point belongs to a particular class, given its features. The theorem is stated as:\n",
        "\n",
        "  $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
        "\n",
        "   Where:\n",
        "\n",
        "    *   $P(A|B)$ is the posterior probability of class A given feature B.\n",
        "    *   $P(B|A)$ is the likelihood of feature B given class A.\n",
        "    *   $P(A)$ is the prior probability of class A.\n",
        "    *   $P(B)$ is the prior probability of feature B.\n",
        "\n",
        "  2.  **Classification:** To classify a new data point, the Naïve Bayes classifier calculates the probability of the data point belonging to each class, using Bayes' Theorem. The data point is then assigned to the class with the highest probability.\n",
        "\n",
        "  The algorithm is called **\"naïve\"** because it makes a simplifying assumption that the features are **conditionally independent** of each other, given the class. In other words, it assumes that the presence or absence of a particular feature does not affect the presence or absence of any other feature, given that we know the class.\n",
        "\n",
        "  For example, in a text classification task, a Naïve Bayes classifier might assume that the probability of the word \"spam\" appearing in an email is independent of the probability of the word \"discount\" appearing in the same email, given that the email is classified as \"spam.\"\n",
        "\n",
        "  This independence assumption is often not true in reality, as features can be correlated. However, despite this \"naïve\" assumption, the classifier often performs well in practice, especially when the independence assumption holds approximately or when the dataset is large. The simplicity of the algorithm and its computational efficiency make it a popular choice for many applications."
      ],
      "metadata": {
        "id": "zoOyP4dPTGMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?**\n",
        "  - There are several variants of the Naïve Bayes classifier, distinguished by the probability distribution assumed for the features. The most common ones are:\n",
        "\n",
        "  1.  **Gaussian Naïve Bayes:**\n",
        "    *   **Description:** Assumes that the features follow a **Gaussian (normal) distribution**. This means the likelihood of a feature value given a class is calculated using the probability density function of the normal distribution.\n",
        "    *   **When to use:** It is typically used for **continuous numerical features** that are assumed to be normally distributed. For example, in classifying emails, if you use the length of the email as a feature, and you assume the lengths of emails in each class follow a normal distribution, you would use Gaussian Naïve Bayes.\n",
        "\n",
        "  2.  **Multinomial Naïve Bayes:**\n",
        "    *   **Description:** Assumes that the features represent **counts or frequencies** of events. It is most commonly used for **text classification**, where features are typically word counts or term frequencies. It follows a multinomial distribution.\n",
        "    *   **When to use:** It is suitable for data where features are discrete counts, such as in document classification (e.g., spam filtering), where the features are the counts of words in a document.\n",
        "\n",
        "  3.  **Bernoulli Naïve Bayes:**\n",
        "    *   **Description:** Assumes that the features are **binary (Boolean) variables**, meaning they can only take two values, typically 0 or 1. It models the presence or absence of a particular feature.\n",
        "    *   **When to use:** It is useful for data where features are indicators of whether a particular event occurred or not. In text classification, for example, it can be used to indicate whether a specific word is present in a document (1) or not (0), regardless of its frequency.\n",
        "\n",
        "  In summary:\n",
        "\n",
        "  *   Use **Gaussian Naïve Bayes** for continuous, normally distributed data.\n",
        "  *   Use **Multinomial Naïve Bayes** for discrete count data, like word counts in text.\n",
        "  *   Use **Bernoulli Naïve Bayes** for binary data, indicating the presence or absence of a feature.\n",
        "\n",
        "  The choice of which Naïve Bayes variant to use depends on the nature of your data and the type of features you have."
      ],
      "metadata": {
        "id": "ZEyLnsFTTPng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Info:**\n",
        "\n",
        "**● You can use any suitable datasets like Iris, Breast Cancer, or Wine from sklearn.datasets or a CSV file you have.**\n",
        "\n",
        "**Q.6. Write a Python program to:**\n",
        "\n",
        "**● Load the Iris dataset**\n",
        "\n",
        "**● Train an SVM Classifier with a linear kernel**\n",
        "\n",
        "**● Print the model's accuracy and support vectors.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "  -"
      ],
      "metadata": {
        "id": "QHwe4q4zTaNO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8df8909",
        "outputId": "2bcd5746-8d81-40ae-bba5-be0983d8a361"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the linear SVM classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_linear.support_vectors_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the linear SVM classifier: 1.00\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.7. Write a Python program to:**\n",
        "\n",
        "**● Load the Breast Cancer dataset**\n",
        "\n",
        "**● Train a Gaussian Naïve Bayes model**\n",
        "\n",
        "**● Print its classification report including precision, recall, and F1-score.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "  -"
      ],
      "metadata": {
        "id": "nNkBzBVDUrm2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08ae0450",
        "outputId": "f0936345-ce46-4c31-acb5-054fcf19185c"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.8. Write a Python program to:**\n",
        "\n",
        "**● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.**\n",
        "\n",
        "**● Print the best hyperparameters and accuracy.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "YsKPP_zRVf9w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c7df4bc",
        "outputId": "7eee870c-8c15-4557-a8fa-e796962bba45"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001],\n",
        "              'kernel': ['rbf']} # Using RBF kernel as it's common for GridSearchCV\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set using the best estimator\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy of the best SVM model: {accuracy:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "Best Hyperparameters found by GridSearchCV:\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Accuracy of the best SVM model: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.9. Write a Python program to:**\n",
        "\n",
        "**● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).**\n",
        "\n",
        "**● Print the model's ROC-AUC score for its predictions.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "Wrjhqct1WS7l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a93fd6",
        "outputId": "ce91b933-1d8c-4667-fa57-c7403534db7e"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a subset of the 20 newsgroups dataset for binary classification\n",
        "categories = ['alt.atheism', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
        "X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model (suitable for text data)\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the predicted probabilities for the positive class\n",
        "y_pred_proba = mnb.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Optional: Plot the ROC curve\n",
        "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "# plt.figure()\n",
        "# plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "# plt.plot([0, 1], [0, 1], 'k--')\n",
        "# plt.xlim([0.0, 1.0])\n",
        "# plt.ylim([0.0, 1.05])\n",
        "# plt.xlabel('False Positive Rate')\n",
        "# plt.ylabel('True Positive Rate')\n",
        "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "# plt.legend(loc=\"lower right\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.10. Imagine you're working as a data scientist for a company that handles email communications.**\n",
        "\n",
        "**Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:**\n",
        "\n",
        "**● Text with diverse vocabulary**\n",
        "\n",
        "**● Potential class imbalance (far more legitimate emails than spam)**\n",
        "\n",
        "**● Some incomplete or missing data**\n",
        "\n",
        "**Explain the approach you would take to:**\n",
        "\n",
        "**● Preprocess the data (e.g. text vectorization, handling missing data)**\n",
        "\n",
        "**● Choose and justify an appropriate model (SVM vs. Naïve Bayes)**\n",
        "\n",
        "**● Address class imbalance**\n",
        "\n",
        "**● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "  - Here's an approach to automatically classify emails as Spam or Not Spam, considering the challenges of diverse vocabulary, class imbalance, and missing data:\n",
        "\n",
        "**1. Data Preprocessing:**\n",
        "\n",
        "*   **Handling Missing Data:** Email data can have missing values in various fields (e.g., sender information, subject, body). Depending on the nature of the missing data, you could:\n",
        "    *   **Impute:** Replace missing values with a placeholder (e.g., \"unknown\") for categorical features, or with the mean/median for numerical features (though less common in email text classification).\n",
        "    *   **Remove:** If missing data is pervasive in certain features and those features are not critical, you might consider removing those features.\n",
        "    *   **Indicator:** Create a binary indicator variable to denote the presence of a missing value in a particular field.\n",
        "*   **Text Vectorization:** To use machine learning models with text data, you need to convert the text into numerical features. Common techniques include:\n",
        "    *   **Bag-of-Words (BoW):** Represents each email as a vector where each dimension corresponds to a word in the vocabulary, and the value is the count of that word in the email.\n",
        "    *   **TF-IDF (Term Frequency-Inverse Document Frequency):** Similar to BoW, but it weights words based on their importance in a document relative to the entire corpus. This helps to downweight common words that appear in many emails.\n",
        "    *   **Word Embeddings (e.g., Word2Vec, GloVe):** These techniques learn dense vector representations of words that capture semantic relationships. While more complex, they can improve performance on tasks requiring understanding of context.\n",
        "    *   **Preprocessing Steps for Text:** Before vectorization, apply standard text preprocessing steps:\n",
        "        *   **Lowercasing:** Convert all text to lowercase.\n",
        "        *   **Punctuation Removal:** Remove punctuation marks.\n",
        "        *   **Stop Word Removal:** Remove common words (e.g., \"the\", \"a\", \"is\") that don't carry much meaning.\n",
        "        *   **Stemming or Lemmatization:** Reduce words to their root form (e.g., \"running\" -> \"run\").\n",
        "\n",
        "**2. Model Selection and Justification (SVM vs. Naïve Bayes):**\n",
        "\n",
        "Both SVM and Naïve Bayes are suitable for text classification, but they have different strengths:\n",
        "\n",
        "*   **Naïve Bayes (Specifically Multinomial Naïve Bayes):**\n",
        "    *   **Pros:** Simple, fast to train and predict, works well with high-dimensional sparse data (like text), and is a good baseline model. It's particularly effective when the independence assumption (features are conditionally independent given the class) holds reasonably well.\n",
        "    *   **Cons:** The independence assumption is often violated in real-world text.\n",
        "    *   **Justification:** Given the diverse vocabulary and the nature of text data (word counts/frequencies), Multinomial Naïve Bayes is a strong initial choice due to its simplicity and efficiency. It can provide a solid baseline performance.\n",
        "\n",
        "*   **Support Vector Machine (SVM):**\n",
        "    *   **Pros:** Powerful for high-dimensional data, can find complex decision boundaries using the kernel trick, and often performs well in practice.\n",
        "    *   **Cons:** Can be computationally more expensive to train than Naïve Bayes, especially with large datasets and complex kernels. Can be sensitive to the choice of kernel and hyperparameters.\n",
        "    *   **Justification:** If Naïve Bayes performance is not satisfactory, SVM with a suitable kernel (like the RBF kernel) can capture more complex relationships between features and potentially achieve higher accuracy. It's a good option for exploring non-linear separation in the feature space.\n",
        "\n",
        "**Recommended Approach:** Start with a Multinomial Naïve Bayes model as a baseline. If needed, then experiment with an SVM model, possibly with GridSearchCV to tune hyperparameters.\n",
        "\n",
        "**3. Addressing Class Imbalance:**\n",
        "\n",
        "Class imbalance (many more legitimate emails than spam) can lead to a model that is biased towards the majority class (not spam). Techniques to address this include:\n",
        "\n",
        "*   **Resampling Techniques:**\n",
        "    *   **Oversampling:** Duplicate instances of the minority class (spam) to balance the dataset. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can generate synthetic minority class samples.\n",
        "    *   **Undersampling:** Randomly remove instances from the majority class (not spam) to match the number of minority class instances. This can lead to loss of information.\n",
        "*   **Class Weighting:** Many machine learning algorithms (including SVM and some Naïve Bayes implementations) allow you to assign higher weights to the minority class during training. This penalizes misclassifications of the minority class more heavily.\n",
        "*   **Using Appropriate Evaluation Metrics:** Avoid relying solely on accuracy when dealing with class imbalance (see below).\n",
        "\n",
        "**4. Evaluating Performance with Suitable Metrics:**\n",
        "\n",
        "Accuracy can be misleading with class imbalance. Instead, use metrics that provide a more nuanced view of performance:\n",
        "\n",
        "*   **Precision:** Of all the emails classified as spam, what proportion were actually spam? (Minimizes false positives - classifying legitimate email as spam).\n",
        "*   **Recall (Sensitivity):** Of all the actual spam emails, what proportion were correctly classified as spam? (Minimizes false negatives - classifying spam as legitimate email).\n",
        "*   **F1-Score:** The harmonic mean of precision and recall, providing a single metric that balances both.\n",
        "*   **ROC-AUC (Receiver Operating Characteristic - Area Under Curve):** Measures the ability of the model to distinguish between the classes. A higher AUC indicates better performance.\n",
        "\n",
        "**5. Business Impact of the Solution:**\n",
        "\n",
        "Implementing an effective spam classification solution can have significant business impact:\n",
        "\n",
        "*   **Increased User Productivity:** Users spend less time dealing with unwanted spam, allowing them to focus on important emails.\n",
        "*   **Improved Security:** Reduces the risk of users clicking on malicious links or opening infected attachments in spam emails.\n",
        "*   **Reduced Infrastructure Costs:** Less storage and processing power are needed to handle spam emails.\n",
        "*   **Enhanced User Experience:** Users have a cleaner and more relevant inbox.\n",
        "*   **Protection of Brand Reputation:** Prevents the company's email system from being used to send spam, which could damage its reputation.\n",
        "\n",
        "By implementing a robust spam classification system, the company can improve efficiency, security, and user satisfaction, leading to a positive business impact."
      ],
      "metadata": {
        "id": "Q2yXKyqvXPyu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d88cbbd",
        "outputId": "72af2367-1417-4bfb-c055-37979d0c0f1f"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Create a synthetic dataset (replace with your actual email data)\n",
        "data = {'email_text': [\"This is a legitimate email about a meeting.\",\n",
        "                       \"Buy now and get a free gift!\",\n",
        "                       \"Meeting rescheduled to 3 PM.\",\n",
        "                       \"Spammy subject line: Urgent financial matter\",\n",
        "                       \"Hello, let's discuss the project.\",\n",
        "                       \" nigeria lottery winner click here\",\n",
        "                       \"Project update attached.\",\n",
        "                       \"Free money no strings attached\"],\n",
        "        'label': ['not spam', 'spam', 'not spam', 'spam', 'not spam', 'spam', 'not spam', 'spam']}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df['email_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) # Using a larger test size for this small example\n",
        "\n",
        "# Text Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mnb.predict(X_test_tfidf)\n",
        "y_pred_proba = mnb.predict_proba(X_test_tfidf)[:, 1] # Probability for the 'spam' class\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Calculate ROC-AUC if applicable (needs binary labels)\n",
        "# Convert labels to binary for ROC-AUC calculation\n",
        "y_test_binary = [1 if label == 'spam' else 0 for label in y_test]\n",
        "y_pred_proba_binary = [proba for proba in y_pred_proba]\n",
        "\n",
        "try:\n",
        "    roc_auc = roc_auc_score(y_test_binary, y_pred_proba_binary)\n",
        "    print(f\"\\nROC-AUC Score: {roc_auc:.2f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"\\nCould not calculate ROC-AUC: {e}\")\n",
        "    print(\"ROC-AUC requires at least one instance of each class in the test set.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not spam       0.25      1.00      0.40         1\n",
            "        spam       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.25         4\n",
            "   macro avg       0.12      0.50      0.20         4\n",
            "weighted avg       0.06      0.25      0.10         4\n",
            "\n",
            "\n",
            "ROC-AUC Score: 0.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}